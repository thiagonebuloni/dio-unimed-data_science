{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Modelo de machine learning para separação de fotos<br>\n",
    "    em categorias de gato ou cachorro </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtVxuuOdvaFx",
    "outputId": "7106ef25-c29d-49fd-e719-d4d3a4257476"
   },
   "outputs": [],
   "source": [
    "\"\"\"!pip install keras\n",
    "!pip install tensorflow\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3p-OjhDPYoZm"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import load_img, img_to_array \n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setando diretório raiz e porcentagens de treinamento e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OGRcLNwYoZu",
    "outputId": "cc619ac5-ac4c-4b7b-b544-fff740d10570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PetImages\\\\Cat', 'PetImages\\\\Dog']\n"
     ]
    }
   ],
   "source": [
    "root = 'PetImages'\n",
    "train_split, val_split = 0.7, 0.15\n",
    "\n",
    "categories = [x[0] for x in os.walk(root) if x[0]][1:]\n",
    "\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2ERhVlFYoZy"
   },
   "source": [
    "Função para pré-processamento das imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A1T1Joq7YoZz"
   },
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    img = load_img(path, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return img, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i3UIwmQtNcb"
   },
   "source": [
    "Removendo arquivos corrompidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86VX9iantE30",
    "outputId": "8a7c6df6-9dbb-40d4-f28e-0f7ddba74e2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dog/11702.jpg', 'Cat/666.jpg']\n"
     ]
    }
   ],
   "source": [
    "removidos = []\n",
    "\n",
    "for arquivo in os.listdir(f'{root}/Dog'):\n",
    "  tamanho = os.path.getsize(f'{root}/Dog/{arquivo}')\n",
    "  if tamanho == 0:\n",
    "    os.remove(f'{root}/Dog/{arquivo}')\n",
    "    removidos.append(f'Dog/{arquivo}')\n",
    "  else:\n",
    "    pass\n",
    "for arquivo in os.listdir(f'{root}/Cat'):\n",
    "  tamanho = os.path.getsize(f'{root}/Cat/{arquivo}')\n",
    "  if tamanho == 0:\n",
    "    os.remove(f'{root}/Cat/{arquivo}')\n",
    "    removidos.append(f'Cat/{arquivo}')\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "print(removidos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUwQ60GGYoZ3"
   },
   "source": [
    "Carregando as imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for c, category in enumerate(categories):\n",
    "    images = [os.path.join(dp, f) for dp, dn, filenames \n",
    "              in os.walk(category) for f in filenames \n",
    "              if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
    "    for img_path in images:\n",
    "        img, x = get_image(img_path)\n",
    "        data.append({'x':np.array(x[0]), 'y':c})\n",
    "\n",
    "# contando o numero de classes\n",
    "num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpiiyrUjMXiI",
    "outputId": "b1411f8c-2d42-42f0-9bb4-0c58ac51a424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([[[ -16.939003 ,   47.221    ,   79.32     ],\n",
      "        [ -14.939003 ,   49.221    ,   81.32     ],\n",
      "        [ -11.939003 ,   52.221    ,   84.32     ],\n",
      "        ...,\n",
      "        [  17.060997 ,   86.221    ,  120.32     ],\n",
      "        [  19.060997 ,   85.221    ,  117.32     ],\n",
      "        [  17.060997 ,   83.221    ,  115.32     ]],\n",
      "\n",
      "       [[ -16.939003 ,   47.221    ,   79.32     ],\n",
      "        [ -14.939003 ,   49.221    ,   81.32     ],\n",
      "        [ -11.939003 ,   52.221    ,   84.32     ],\n",
      "        ...,\n",
      "        [  18.060997 ,   87.221    ,  121.32     ],\n",
      "        [  19.060997 ,   85.221    ,  117.32     ],\n",
      "        [  18.060997 ,   84.221    ,  116.32     ]],\n",
      "\n",
      "       [[ -16.939003 ,   47.221    ,   79.32     ],\n",
      "        [ -14.939003 ,   49.221    ,   81.32     ],\n",
      "        [ -11.939003 ,   52.221    ,   84.32     ],\n",
      "        ...,\n",
      "        [  18.060997 ,   88.221    ,  119.32     ],\n",
      "        [  20.060997 ,   86.221    ,  118.32     ],\n",
      "        [  18.060997 ,   84.221    ,  116.32     ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ -47.939003 ,    6.2210007,   30.32     ],\n",
      "        [ -46.939003 ,    7.2210007,   31.32     ],\n",
      "        [ -45.939003 ,    8.221001 ,   32.32     ],\n",
      "        ...,\n",
      "        [-102.939    , -113.779    , -120.68     ],\n",
      "        [-102.939    , -113.779    , -120.68     ],\n",
      "        [-102.939    , -113.779    , -120.68     ]],\n",
      "\n",
      "       [[ -48.939003 ,    5.2210007,   29.32     ],\n",
      "        [ -48.939003 ,    5.2210007,   29.32     ],\n",
      "        [ -47.939003 ,    6.2210007,   30.32     ],\n",
      "        ...,\n",
      "        [-103.939    , -114.779    , -121.68     ],\n",
      "        [-103.939    , -114.779    , -121.68     ],\n",
      "        [-103.939    , -114.779    , -121.68     ]],\n",
      "\n",
      "       [[ -50.939003 ,    3.2210007,   27.32     ],\n",
      "        [ -49.939003 ,    4.2210007,   28.32     ],\n",
      "        [ -48.939003 ,    5.2210007,   29.32     ],\n",
      "        ...,\n",
      "        [-103.939    , -115.779    , -122.68     ],\n",
      "        [-103.939    , -115.779    , -122.68     ],\n",
      "        [-103.939    , -115.779    , -122.68     ]]], dtype=float32), 'y': 0}\n"
     ]
    }
   ],
   "source": [
    "print(data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55Rw-ptVYoZ7"
   },
   "source": [
    "Randomizando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5vGeJK55YoZ8"
   },
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwHqS_NgYoZ_"
   },
   "source": [
    "Separando dados de treinamento, validação e teste (70%, 15%, 15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_val = int(train_split * len(data))\n",
    "idx_test = int((train_split + val_split) * len(data))\n",
    "train = data[:idx_val]\n",
    "val = data[idx_val:idx_test]\n",
    "test = data[idx_test:]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsOVhpqcYoaF"
   },
   "source": [
    "Separando dados por rótulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQOGN9kOYoaH"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = np.array([t[\"x\"] for t in train]), [t[\"y\"] for t in train]\n",
    "x_val, y_val = np.array([t[\"x\"] for t in val]), [t[\"y\"] for t in val]\n",
    "x_test, y_test = np.array([t[\"x\"] for t in test]), [t[\"y\"] for t in test]\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc6W07QVYoaM"
   },
   "source": [
    "### Pré-processando os dados, garantindo float32 e normalizando em 0 e 1\n",
    "Caso acuse erro de memória pule essa etapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnXaiAgJYoaQ"
   },
   "outputs": [],
   "source": [
    "# normalizando\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_val = x_val.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ordUucUKYoaS"
   },
   "source": [
    "Resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcKjxgtyYoaT"
   },
   "outputs": [],
   "source": [
    "# summary\n",
    "print(\"Carregadas %d imagens de %d categorias\"%(len(data), num_classes))\n",
    "print(\"treino / validação / teste: %d, %d, %d\"%(len(x_train), len(x_val), len(x_test)))\n",
    "print(\"shape dos dados treinados: \", x_train.shape)\n",
    "print(\"shape dos rótulos treinados: \", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando imagens de amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y84SmM2CYoaZ"
   },
   "outputs": [],
   "source": [
    "images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
    "idx = [int(len(images) * random.random()) for i in range(8)]\n",
    "imgs = [image.load_img(images[i], target_size=(224, 224)) for i in idx]\n",
    "concat_image = np.concatenate([np.asarray(img) for img in imgs], axis=1)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.imshow(concat_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2s5qypkYoad"
   },
   "source": [
    "### Criando redes neurais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y41GiiYTYoaf"
   },
   "outputs": [],
   "source": [
    "# build the network\n",
    "model = Sequential()\n",
    "print(\"Input dimensions: \",x_train.shape[1:])\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIqHecNAYoaj"
   },
   "outputs": [],
   "source": [
    "# compile the model to use categorical cross-entropy loss function and adadelta optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG0CKOI1Yoao"
   },
   "source": [
    "Plotando validação de loss e acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CCPq_ndYoap"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,4))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(history.history[\"val_loss\"])\n",
    "ax.set_title(\"validation loss\")\n",
    "ax.set_xlabel(\"epochs\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(history.history[\"val_acc\"])\n",
    "ax2.set_title(\"validation accuracy\")\n",
    "ax2.set_xlabel(\"epochs\")\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Itd5LDAYoav"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIwMY_ZXYoax"
   },
   "source": [
    "## Transfer learning com rede já existente\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpUDAbxiYoay"
   },
   "outputs": [],
   "source": [
    "vgg = keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFL-fLitYoa3"
   },
   "outputs": [],
   "source": [
    "# make a reference to VGG's input layer\n",
    "inp = vgg.input\n",
    "\n",
    "# make a new softmax layer with num_classes neurons\n",
    "new_classification_layer = Dense(num_classes, activation='softmax')\n",
    "\n",
    "# connect our new layer to the second to last layer in VGG, and make a reference to it\n",
    "out = new_classification_layer(vgg.layers[-2].output)\n",
    "\n",
    "# create a new network between inp and out\n",
    "model_new = Model(inp, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_n5A8oGYoa9"
   },
   "outputs": [],
   "source": [
    "# make all layers untrainable by freezing weights (except for last layer)\n",
    "for l, layer in enumerate(model_new.layers[:-1]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# ensure the last layer is trainable/not frozen\n",
    "for l, layer in enumerate(model_new.layers[-1:]):\n",
    "    layer.trainable = True\n",
    "\n",
    "model_new.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDdq71XNYobD"
   },
   "outputs": [],
   "source": [
    "history2 = model_new.fit(x_train, y_train, \n",
    "                         batch_size=128, \n",
    "                         epochs=10, \n",
    "                         validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHLdHnuuYobJ"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,4))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(history.history[\"val_loss\"])\n",
    "ax.plot(history2.history[\"val_loss\"])\n",
    "ax.set_title(\"validation loss\")\n",
    "ax.set_xlabel(\"epochs\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(history.history[\"val_acc\"])\n",
    "ax2.plot(history2.history[\"val_acc\"])\n",
    "ax2.set_title(\"validation accuracy\")\n",
    "ax2.set_xlabel(\"epochs\")\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMxC6Pd1YobN"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iUykardYobR"
   },
   "source": [
    "Predizendo novas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpRcsywEYobT"
   },
   "outputs": [],
   "source": [
    "img, x = get_image('101_ObjectCategories/airplanes/image_0003.jpg')\n",
    "probabilities = model_new.predict([x])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
